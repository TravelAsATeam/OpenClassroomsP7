{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Ce notebook a pour objectif de rechercher le meilleur modèle de prédiction pour des données déjà traitées. Un des points importants qu'ilm conviendra de résoudre est le déséquilibre entre les classes. Pöur résoudre ce problème, plusieurs possibilités s'offrent à nous. Il conviendra de tester différentes méthodes mais également de tester des combinaisons de méthodes.\n",
    "\n",
    " Méthodes de gestion du déséquilibre de classe et un exemple :\n",
    "     \n",
    "     - choix de métriques adaptées (Score Fbeta)\n",
    "     \n",
    "     - pondération des erreurs en fonction du déséquilibre des classes (class_weight)\n",
    "     \n",
    "     - choix d'algorithmes adaptés (Random forest)\n",
    "     \n",
    "     - Sous ou sur échantillonnage (SMOTE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choix des métriques adaptées :\n",
    "Dans le cas de classes déséquilibrées, il est fréquent que toutes les erreurs ne se valent pas. Ici, les évènements positifs (défaut de paiement) son bien plus rares que les évènements négatifs. D'un point de vue purement rentable, il vaut mieux faire l'erreur de ne pas octroyer de prêt à un bon payeur que d'en octroyer un à un mauvais payeur. Avoir des faux positifs est donc plus désirable que d'avoir des faux négatifs.\n",
    "\n",
    "La question est plus complexe s'il on intègre des notions d'éthique, puisque refuser un prêt à un bon payeur à des conséquences négatives, ainsi il faudrait discuter des problématiques éthiques à refuser l'ocrtoi d'un prêt à un bon payeur ou en octroyer un à un mauvais payeur. Il faudrait ensuite calculer un coefficient de non-désirabilité qui permettrait d'évaluer quelle est l'importance des faux positifs par rapport aux faux négatifs.\n",
    "\n",
    "C'est une question complexe qui demande une réflexion éthique et économique.\n",
    "\n",
    "Pour l'exercice, nous fixerons le rapport de non désirabilité à cette règle : un faux négatif a un impact négatif 10 fois plus important qu'un faux positif.\n",
    "\n",
    "A partir de cette règle on peut déterminer le paramètre beta d'un F-beta score qui permettra d'adapter la métrique d'évaluation au problème de déséquilibre des classes et de la différence d'impact des erreurs. Dans ce cas, c'est le recall qui aura plus d'importance que la precision. beta = racine_carrée(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pondération  des erreurs :\n",
    "La méthode de pondération des erreurs en fonction du déséquilibre des classes consiste à considérer les erreurs effectuées sur la classe mineure comme plus importantes que celles commises sur la classe majeure. Le ratio qui relie ces importances peut correspondre au ratio du nombre d'observations entre la classe mineure et la classe majeure.\n",
    "\n",
    "Il me semble que cette méthode est redondante avec le choix d'un F-beta score bien que le F-beta soit conçu pour refléter la gravité d'une erreur plutôt qu'une autre alors que le class_weight est conçu pour s'adapter au déséquilibre dans le nombre d'observations. On testera son effet pour le vérifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stratégie \n",
    "\n",
    "En utilisant une métrique F-beta score pondérée pour donner plus d'importance aux faux négatifs (qui sont des erreurs plus importantes), on va rechercher les meilleurs hyper paramètres sur 4 types d'algorithmes différents en utilisant la recherche par grille avec validation croisée et en utilisant le jeu de données sous-échantillonnée pour accélérer les calculs. \n",
    "\n",
    "Ensuite, une fois les hyper paramètres trouvés, on testera les 3 jeux de données avec chaque algorithme et on consultera la courbe ROC pour d'éventuelles optimisations du seuil. Une fois le modèle optimal choisi, on testera l'impact du class_weight. On terminera par une synthèse des modèles pour chaque jeu de données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: imbalanced-learn in g:\\softwares\\anaconda3\\lib\\site-packages (0.8.1)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=0.19.1 in g:\\softwares\\anaconda3\\lib\\site-packages (from imbalanced-learn) (1.5.2)\n",
      "Requirement already satisfied, skipping upgrade: joblib>=0.11 in g:\\softwares\\anaconda3\\lib\\site-packages (from imbalanced-learn) (0.17.0)\n",
      "Requirement already satisfied, skipping upgrade: scikit-learn>=0.24 in g:\\softwares\\anaconda3\\lib\\site-packages (from imbalanced-learn) (1.0.1)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.13.3 in g:\\softwares\\anaconda3\\lib\\site-packages (from imbalanced-learn) (1.19.2)\n",
      "Requirement already satisfied, skipping upgrade: threadpoolctl>=2.0.0 in g:\\softwares\\anaconda3\\lib\\site-packages (from scikit-learn>=0.24->imbalanced-learn) (2.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from time import time\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Affichage de toutes les colonnes et lignes lorsque demandé\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import du fichier de données pré-traitées\n",
    "default_dir = \"G:/OCDataScientist/Projet7\"\n",
    "data = pd.read_csv(os.path.join(default_dir,'data_train.csv'))\n",
    "SK_ID_CURR = data.pop('SK_ID_CURR')\n",
    "\n",
    "# Création d'un imputer pour remplacer les valeurs manquantes\n",
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "# Remplacement des valeurs manquantes\n",
    "temp_DF = pd.DataFrame(imputer.fit_transform(data))\n",
    "# Réintégration des colonnes et index\n",
    "temp_DF.columns = data.columns\n",
    "temp_DF.index = data.index\n",
    "data = temp_DF\n",
    "\n",
    "# Création d'un scaler 0-1\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range = (0, 1))\n",
    "scaler.fit(data)\n",
    "\n",
    "# Normalisation des variables\n",
    "temp_DF = pd.DataFrame(scaler.transform(data))\n",
    "# Réintégration des colonnes et index\n",
    "temp_DF.columns = data.columns\n",
    "temp_DF.index = data.index\n",
    "data = temp_DF\n",
    "\n",
    "# Sauvegarde du DataFrame des données prêtes à être modélisées\n",
    "#Export des fichiers de données traités\n",
    "output_dir = \"G:/OCDataScientist/Projet7\"\n",
    "data.to_csv(os.path.join(output_dir,'data_train_std.csv'), index=False)\n",
    "\n",
    "# Décomposition train/test\n",
    "y = data['TARGET'].values\n",
    "X = data.drop(columns = ['TARGET']).values\n",
    "from sklearn import model_selection\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.3,\n",
    "                                                                    random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affichage des classes avec t-sne\n",
    "from sklearn import manifold\n",
    "pca = manifold.TSNE(n_components=2, perplexity=30, n_iter=5000, init='pca')\n",
    "# Sous échantillonnage pour réduire les temps de calcul\n",
    "data_sample = data.sample(n=5000)\n",
    "y_sample = data_sample['TARGET'].values\n",
    "X_sample = data_sample.drop(columns = ['TARGET']).values\n",
    "\n",
    "X_pca = pca.fit_transform(X_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Graphique de visualisation des classes selon deux composantes t-sne\n",
    "plt.figure(figsize=(16,16))\n",
    "plt.scatter(X_pca[:,0],X_pca[:,1],c=y_sample.astype(np.float), cmap = 'jet', alpha=.2)\n",
    "plt.title('Visualisation t-sne des classes', fontsize = 15, pad = 35, fontweight = 'bold')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si des clusters se démarquent, ils ne correspondent pas aux classes. Il semble qu'une classification efficace sera très compliquée à mettre en oeuvre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Répartition originale: Counter({0.0: 197772, 1.0: 17485})\n",
      "Répartition après sous-échantillonnage Counter({0.0: 17485, 1.0: 17485})\n"
     ]
    }
   ],
   "source": [
    "# Sous-échantillonnage\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "rus = RandomUnderSampler(random_state=0, replacement=True)\n",
    "X_train_rus, y_train_rus = rus.fit_resample(X_train, y_train)\n",
    "\n",
    "# Vérification\n",
    "from collections import Counter\n",
    "print('Répartition originale:', Counter(y_train))\n",
    "print('Répartition après sous-échantillonnage', Counter(y_train_rus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sur-échantillonnage SMOTE\n",
    "from imblearn.over_sampling import SMOTE\n",
    "smote = SMOTE()\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Vérification\n",
    "from collections import Counter\n",
    "print('Répartition originale:', Counter(y_train))\n",
    "print('Répartition après sur-échantillonnage', Counter(y_train_smote))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création d'une métrique adaptée Fbeta score pour le GridSearchCV\n",
    "from sklearn import metrics\n",
    "target = metrics.make_scorer(metrics.fbeta_score, beta=3.16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "# Pour le modèle KNN, les paramètres par défauts conviendront.\n",
    "\n",
    "# Entrainement sur le jeu de données déséquilibré\n",
    "knn_imb = KNeighborsClassifier(n_jobs = -1)\n",
    "start = time()\n",
    "knn_imb.fit(X_train, y_train)\n",
    "end = time()\n",
    "time_knn_imb_train = end - start\n",
    "print(\"Training Time: {:.2f}\".format(time_knn_imb_train))\n",
    "# Prédiction\n",
    "start = time()\n",
    "y_knn_imb_pred = knn_imb.predict(X_test)\n",
    "end = time()\n",
    "time_knn_imb_pred = end - start\n",
    "print(\"Testing Time: {:.2f}\".format(time_knn_imb_pred))\n",
    "# Métriques\n",
    "acc_knn_imb = metrics.accuracy_score(y_test, y_knn_imb_pred)\n",
    "print(\"Accuracy: {:.3f}\".format(acc_knn_imb))\n",
    "fbeta_knn_imb = metrics.fbeta_score(y_test, y_knn_imb_pred, beta=3.16)\n",
    "print(\"Fbeta: {:.3f}\".format(fbeta_knn_imb))\n",
    "confusion_knn_imb = metrics.confusion_matrix(y_test, y_knn_imb_pred)\n",
    "print(confusion_knn_imb)\n",
    "print(metrics.classification_report(y_test, y_knn_imb_pred))\n",
    "# Courbe ROC\n",
    "fpr, tpr, _ = metrics.roc_curve(y_test,  knn_imb.predict_proba(X_test)[:,1])\n",
    "plt.plot(fpr,tpr)\n",
    "plt.ylabel('Ratio de Vrai Positif')\n",
    "plt.xlabel('Ratio de Faux Positif')\n",
    "plt.show()\n",
    "\n",
    "# Entrainement sur le jeu de données sous échantillonné\n",
    "knn_rus = KNeighborsClassifier(n_jobs = -1)\n",
    "start = time()\n",
    "knn_rus.fit(X_train_rus, y_train_rus)\n",
    "end = time()\n",
    "time_knn_rus_train = end - start\n",
    "print(\"Training Time: {:.2f}\".format(time_knn_rus_train))\n",
    "# Prédiction\n",
    "start = time()\n",
    "y_knn_rus_pred = knn_rus.predict(X_test)\n",
    "end = time()\n",
    "time_knn_rus_pred = end - start\n",
    "print(\"Testing Time: {:.2f}\".format(time_knn_rus_pred))\n",
    "# Métriques\n",
    "acc_knn_rus = metrics.accuracy_score(y_test, y_knn_rus_pred)\n",
    "print(\"Accuracy: {:.3f}\".format(acc_knn_rus))\n",
    "fbeta_knn_rus = metrics.fbeta_score(y_test, y_knn_rus_pred, beta=3.16)\n",
    "print(\"Fbeta: {:.3f}\".format(fbeta_knn_rus))\n",
    "confusion_knn_rus = metrics.confusion_matrix(y_test, y_knn_rus_pred)\n",
    "print(confusion_knn_rus)\n",
    "print(metrics.classification_report(y_test, y_knn_rus_pred))\n",
    "# Courbe ROC\n",
    "fpr, tpr, _ = metrics.roc_curve(y_test,  knn_rus.predict_proba(X_test)[:,1])\n",
    "plt.plot(fpr,tpr)\n",
    "plt.ylabel('Ratio de Vrai Positif')\n",
    "plt.xlabel('Ratio de Faux Positif')\n",
    "plt.show()\n",
    "\n",
    "# Entrainement sur le jeu de données sur-échantillonné\n",
    "knn_smote = KNeighborsClassifier(n_jobs = -1)\n",
    "start = time()\n",
    "knn_smote.fit(X_train_smote, y_train_smote)\n",
    "end = time()\n",
    "time_knn_smote_train = end - start\n",
    "print(\"Training Time: {:.2f}\".format(time_knn_smote_train))\n",
    "# Prédiction\n",
    "start = time()\n",
    "y_knn_smote_pred = knn_smote.predict(X_test)\n",
    "end = time()\n",
    "time_knn_smote_pred = end - start\n",
    "print(\"Testing Time: {:.2f}\".format(time_knn_smote_pred))\n",
    "# Métriques\n",
    "acc_knn_smote = metrics.accuracy_score(y_test, y_knn_smote_pred)\n",
    "print(\"Accuracy: {:.3f}\".format(acc_knn_smote))\n",
    "fbeta_knn_smote = metrics.fbeta_score(y_test, y_knn_smote_pred, beta=3.16)\n",
    "print(\"Fbeta: {:.3f}\".format(fbeta_knn_smote))\n",
    "confusion_knn_smote = metrics.confusion_matrix(y_test, y_knn_smote_pred)\n",
    "print(confusion_knn_smote)\n",
    "print(metrics.classification_report(y_test, y_knn_smote_pred))\n",
    "# Courbe ROC\n",
    "fpr, tpr, _ = metrics.roc_curve(y_test,  knn_smote.predict_proba(X_test)[:,1])\n",
    "plt.plot(fpr,tpr)\n",
    "plt.ylabel('Ratio de Vrai Positif')\n",
    "plt.xlabel('Ratio de Faux Positif')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sur le jeu de données déséquilibré, on retrouve la précision la plus élevée mais on remarque qu'il ne parvient à détecter seulement 122 TP ce qui est très faible. Le Fbeta résultant qui met l'accent sur les TP est donc très bas. Ici le problème du déséquilibre de classes est flagrant.\n",
    "\n",
    "Sur le jeu sous échantillonné, la précision est la plus faible, il repère en effet beaucoup de FP, cependant c'est sur ce jeu là que l'on trouve le plus de TP, il obtient donc le Fbeta le plus élevé.\n",
    "\n",
    "Sur le jeu sur-échantillonné, on obtient des performances plus faibles sur le nombre de TP, il échoue à détecter plus de la moitié des positifs.\n",
    "\n",
    "Globalement, si le jeu sous-échantillonné s'en sort mieux (en prenant comme objectif Fbeta avec beta=3.16), le modèle KNN présente des performances faibles, il repère au mieux 59% des positifs avec un taux de FP de presque 10%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour la suite de la recherche du meilleur algorithme et pour optimiser la recherche des hyperparamètres, il serait intéressant de réduire la taille de l'échantillon et s'il faut réduire l'échantillon pour accélérer cette phase, alors autant en profiter pour le rééquilibrer. La recherche de l'algorithme et des hyper paramèrtres optimaux se fera donc avec le jeu de données rééquilibré par échantillonnage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression logistique avec GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "paramGrid = {'penalty': ['l1','l2','elasticnet'], 'random_state' : [0],\n",
    "             'class_weight' : [None], 'solver' : ['newton-cg', 'lbfgs', 'liblinear','sag','saga'],\n",
    "             'max_iter' :[1000],'l1_ratio' : [0.5], 'n_jobs':[-1]}\n",
    "\n",
    "# Entrainement sur le jeu de données sous échantillonné\n",
    "start = time()\n",
    "reg_rus = reg = model_selection.GridSearchCV(LogisticRegression(), paramGrid, cv=5,\n",
    "                                             scoring=target)\n",
    "reg_rus.fit(X_train_rus, y_train_rus)\n",
    "end = time()\n",
    "time_reg_grid = end - start\n",
    "print(\"Testing Time: {:.2f}\".format(time_reg_grid))\n",
    "\n",
    "print(reg_rus.best_params_)\n",
    "# Afficher les performances correspondantes\n",
    "print(\"Résultats de la validation croisée :\")\n",
    "for mean, std, params in zip(reg_rus.cv_results_['mean_test_score'],\n",
    "                             reg_rus.cv_results_['std_test_score'],\n",
    "                             reg_rus.cv_results_['params']):\n",
    "    print(\"{} = {:.3f} (+/-{:.03f}) for {}\".format(target,mean,std,params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le meilleur solver par regression logistique semble être le liblinear avec une pénalité l2. Nous allons maintenant le tester avec les 3 jeux de données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression Logistique\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# Entrainement sur le jeu de données déséquilibré\n",
    "reg_imb = LogisticRegression(penalty = 'l2', solver = 'liblinear', random_state = 0,\n",
    "                             max_iter = 1000, n_jobs = -1)\n",
    "start = time()\n",
    "reg_imb.fit(X_train, y_train)\n",
    "end = time()\n",
    "time_reg_imb_train = end - start\n",
    "print(\"Training Time: {:.2f}\".format(time_reg_imb_train))\n",
    "# Prédiction\n",
    "start = time()\n",
    "y_reg_imb_pred = reg_imb.predict(X_test)\n",
    "end = time()\n",
    "time_reg_imb_pred = end - start\n",
    "print(\"Testing Time: {:.2f}\".format(time_reg_imb_pred))\n",
    "# Métriques\n",
    "acc_reg_imb = metrics.accuracy_score(y_test, y_reg_imb_pred)\n",
    "print(\"Accuracy: {:.3f}\".format(acc_reg_imb))\n",
    "fbeta_reg_imb = metrics.fbeta_score(y_test, y_reg_imb_pred, beta=3.16)\n",
    "print(\"Fbeta: {:.3f}\".format(fbeta_reg_imb))\n",
    "confusion_reg_imb = metrics.confusion_matrix(y_test, y_reg_imb_pred)\n",
    "print(confusion_reg_imb)\n",
    "print(metrics.classification_report(y_test, y_reg_imb_pred))\n",
    "# Courbe ROC\n",
    "fpr, tpr, _ = metrics.roc_curve(y_test,  reg_imb.predict_proba(X_test)[:,1])\n",
    "plt.plot(fpr,tpr)\n",
    "plt.ylabel('Ratio de Vrai Positif')\n",
    "plt.xlabel('Ratio de Faux Positif')\n",
    "plt.show()\n",
    "\n",
    "# Entrainement sur le jeu de données sous échantillonné\n",
    "reg_rus = LogisticRegression(penalty = 'l2', solver = 'liblinear', random_state = 0,\n",
    "                             max_iter = 1000, n_jobs = -1)\n",
    "start = time()\n",
    "reg_rus.fit(X_train_rus, y_train_rus)\n",
    "end = time()\n",
    "time_reg_rus_train = end - start\n",
    "print(\"Training Time: {:.2f}\".format(time_reg_rus_train))\n",
    "# Prédiction\n",
    "start = time()\n",
    "y_reg_rus_pred = reg_rus.predict(X_test)\n",
    "end = time()\n",
    "time_reg_rus_pred = end - start\n",
    "print(\"Testing Time: {:.2f}\".format(time_reg_rus_pred))\n",
    "# Métriques\n",
    "acc_reg_rus = metrics.accuracy_score(y_test, y_reg_rus_pred)\n",
    "print(\"Accuracy: {:.3f}\".format(acc_reg_rus))\n",
    "fbeta_reg_rus = metrics.fbeta_score(y_test, y_reg_rus_pred, beta=3.16)\n",
    "print(\"Fbeta: {:.3f}\".format(fbeta_reg_rus))\n",
    "confusion_reg_rus = metrics.confusion_matrix(y_test, y_reg_rus_pred)\n",
    "print(confusion_reg_rus)\n",
    "print(metrics.classification_report(y_test, y_reg_rus_pred))\n",
    "# Courbe ROC\n",
    "fpr, tpr, _ = metrics.roc_curve(y_test,  reg_rus.predict_proba(X_test)[:,1])\n",
    "plt.plot(fpr,tpr)\n",
    "plt.ylabel('Ratio de Vrai Positif')\n",
    "plt.xlabel('Ratio de Faux Positif')\n",
    "plt.show()\n",
    "\n",
    "# Entrainement sur le jeu de données sur-échantillonné\n",
    "reg_smote = LogisticRegression(penalty = 'l2', solver = 'liblinear', random_state = 0,\n",
    "                               max_iter = 1000, n_jobs = -1)\n",
    "start = time()\n",
    "reg_smote.fit(X_train_smote, y_train_smote)\n",
    "end = time()\n",
    "time_reg_smote_train = end - start\n",
    "print(\"Training Time: {:.2f}\".format(time_reg_smote_train))\n",
    "# Prédiction\n",
    "start = time()\n",
    "y_reg_smote_pred = reg_smote.predict(X_test)\n",
    "end = time()\n",
    "time_reg_smote_pred = end - start\n",
    "print(\"Testing Time: {:.2f}\".format(time_reg_smote_pred))\n",
    "# Métriques\n",
    "acc_reg_smote = metrics.accuracy_score(y_test, y_reg_smote_pred)\n",
    "print(\"Accuracy: {:.3f}\".format(acc_reg_smote))\n",
    "fbeta_reg_smote = metrics.fbeta_score(y_test, y_reg_smote_pred, beta=3.16)\n",
    "print(\"Fbeta: {:.3f}\".format(fbeta_reg_smote))\n",
    "confusion_reg_smote = metrics.confusion_matrix(y_test, y_reg_smote_pred)\n",
    "print(confusion_reg_smote)\n",
    "print(metrics.classification_report(y_test, y_reg_smote_pred))\n",
    "# Courbe ROC\n",
    "fpr, tpr, _ = metrics.roc_curve(y_test,  reg_smote.predict_proba(X_test)[:,1])\n",
    "plt.plot(fpr,tpr)\n",
    "plt.ylabel('Ratio de Vrai Positif')\n",
    "plt.xlabel('Ratio de Faux Positif')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La comparaison des jeux de données est assez similaire avec celle que nous avons fait lors du kNN, le jeu déséquilibré apporte une précision très élevée mais ne parvient pas à détecter les TP, le jeu sous-échantillonné présente les meilleurs résultats sur la métrique principale (Fbeta score).\n",
    "\n",
    "Si la regression logistique présente de meilleurs résultats que le kNN, notamment sur une bonne détection des TP, elle échoue en détectant près d'un tiers de faux positifs. Au vu de l'utilisation finale du modèle, ces résultats ne me semblent pas suffisants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random forest avec GridSearch CV\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "# Grilles dans l'ordre de recherche\n",
    "paramGrid1 = {'loss' : ['deviance', 'exponential'], 'max_depth': [3,4,5],\n",
    "             'learning_rate':[0.05,0.1,0.02] ,  'n_estimators':[50,100,200], \n",
    "             'random_state':[0]}\n",
    "paramGrid2 = {'loss' : ['exponential'], 'learning_rate':[0.08,0.1,0.12], 'max_depth': [4],\n",
    "              'n_estimators':[150,200,250], 'random_state':[0]}\n",
    "paramGrid3 = {'loss' : ['exponential'], 'learning_rate': [0.12, 0.15, 0.25] , 'max_depth': [4],\n",
    "              'n_estimators':[250,300,400], 'random_state':[0]}\n",
    "\n",
    "# Entrainement sur le jeu de données sous echantillonné\n",
    "start = time()\n",
    "rf_rus = model_selection.GridSearchCV(GradientBoostingClassifier(), paramGrid1, cv=5,\n",
    "                                      scoring=target)\n",
    "rf_rus.fit(X_train_rus, y_train_rus)\n",
    "end = time()\n",
    "time_rf_grid = end - start\n",
    "print(\"Testing Time: {:.2f}\".format(time_rf_grid))\n",
    "\n",
    "print(rf_rus.best_params_)\n",
    "# Afficher les performances correspondantes\n",
    "print(\"Résultats de la validation croisée :\")\n",
    "for mean, std, params in zip(rf_rus.cv_results_['mean_test_score'], \n",
    "                             rf_rus.cv_results_['std_test_score'], rf_rus.cv_results_['params']):\n",
    "    print(\"{} = {:.3f} (+/-{:.03f}) for {}\".format(target,mean,std,params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En première approche, on a trouvé les meilleurs paramètres tels que : {'learning_rate': 0.1, 'loss': 'exponential', 'max_depth': 4, 'n_estimators': 200, 'random_state': 0}\n",
    "\n",
    "En deuxième approche, {'learning_rate': 0.12, 'loss': 'exponential', 'max_depth': 4, 'n_estimators': 250, 'random_state': 0}\n",
    "\n",
    "En troisième approche, {'learning_rate': 0.12, 'loss': 'exponential', 'max_depth': 4, 'n_estimators': 250, 'random_state': 0} \n",
    "\n",
    "Les mêmes paramètres qu'en deuxième approche.\n",
    "\n",
    "Nous allons maintenant tester ces paramètres avec les 3 jeux de données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Time: 1160.25\n",
      "Testing Time: 0.82\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'metrics' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-3a79c831ea14>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Testing Time: {:.2f}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime_rf_imb_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;31m# Métriques\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[0macc_rf_imb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_rf_imb_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Accuracy: {:.3f}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0macc_rf_imb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[0mfbeta_rf_imb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfbeta_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_rf_imb_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3.16\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'metrics' is not defined"
     ]
    }
   ],
   "source": [
    "# Random Forest avec Gradient Boosting\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "# Entrainement sur le jeu de données déséquilibré\n",
    "rf_imb = GradientBoostingClassifier(learning_rate = 0.12, loss = 'exponential', max_depth = 4, \n",
    "                                    n_estimators = 250, random_state = 0)\n",
    "start = time()\n",
    "rf_imb.fit(X_train, y_train)\n",
    "end = time()\n",
    "time_rf_imb_train = end - start\n",
    "print(\"Training Time: {:.2f}\".format(time_rf_imb_train))\n",
    "# Prédiction\n",
    "start = time()\n",
    "y_rf_imb_pred = rf_imb.predict(X_test)\n",
    "end = time()\n",
    "time_rf_imb_pred = end - start\n",
    "print(\"Testing Time: {:.2f}\".format(time_rf_imb_pred))\n",
    "# Métriques\n",
    "acc_rf_imb = metrics.accuracy_score(y_test, y_rf_imb_pred)\n",
    "print(\"Accuracy: {:.3f}\".format(acc_rf_imb))\n",
    "fbeta_rf_imb = metrics.fbeta_score(y_test, y_rf_imb_pred, beta=3.16)\n",
    "print(\"Fbeta: {:.3f}\".format(fbeta_rf_imb))\n",
    "confusion_rf_imb = metrics.confusion_matrix(y_test, y_rf_imb_pred)\n",
    "print(confusion_rf_imb)\n",
    "print(metrics.classification_report(y_test, y_rf_imb_pred))\n",
    "# Courbe ROC\n",
    "fpr, tpr, _ = metrics.roc_curve(y_test,  rf_imb.predict_proba(X_test)[:,1])\n",
    "plt.plot(fpr,tpr)\n",
    "plt.ylabel('Ratio de Vrai Positif')\n",
    "plt.xlabel('Ratio de Faux Positif')\n",
    "plt.show()\n",
    "\n",
    "# Entrainement sur le jeu de données sous échantillonné\n",
    "rf_rus = GradientBoostingClassifier(learning_rate = 0.12, loss = 'exponential', max_depth = 4, \n",
    "                                    n_estimators = 250, random_state = 0)\n",
    "start = time()\n",
    "rf_rus.fit(X_train_rus, y_train_rus)\n",
    "end = time()\n",
    "time_rf_rus_train = end - start\n",
    "print(\"Training Time: {:.2f}\".format(time_rf_rus_train))\n",
    "# Prédiction\n",
    "start = time()\n",
    "y_rf_rus_pred = rf_rus.predict(X_test)\n",
    "end = time()\n",
    "time_rf_rus_pred = end - start\n",
    "print(\"Testing Time: {:.2f}\".format(time_rf_rus_pred))\n",
    "# Métriques\n",
    "acc_rf_rus = metrics.accuracy_score(y_test, y_rf_rus_pred)\n",
    "print(\"Accuracy: {:.3f}\".format(acc_rf_rus))\n",
    "fbeta_rf_rus = metrics.fbeta_score(y_test, y_rf_rus_pred, beta=3.16)\n",
    "print(\"Fbeta: {:.3f}\".format(fbeta_rf_rus))\n",
    "confusion_rf_rus = metrics.confusion_matrix(y_test, y_rf_rus_pred)\n",
    "print(confusion_rf_rus)\n",
    "print(metrics.classification_report(y_test, y_rf_rus_pred))\n",
    "# Courbe ROC\n",
    "fpr, tpr, _ = metrics.roc_curve(y_test,  rf_rus.predict_proba(X_test)[:,1])\n",
    "plt.plot(fpr,tpr)\n",
    "plt.ylabel('Ratio de Vrai Positif')\n",
    "plt.xlabel('Ratio de FAux Positif')\n",
    "plt.show()\n",
    "\n",
    "# Entrainement sur le jeu de données sur-échantillonné\n",
    "rf_smote = GradientBoostingClassifier(learning_rate = 0.12, loss = 'exponential', max_depth = 4, \n",
    "                                    n_estimators = 250, random_state = 0)\n",
    "start = time()\n",
    "rf_smote.fit(X_train_smote, y_train_smote)\n",
    "end = time()\n",
    "time_rf_smote_train = end - start\n",
    "print(\"Training Time: {:.2f}\".format(time_rf_smote_train))\n",
    "# Prédiction\n",
    "start = time()\n",
    "y_rf_smote_pred = rf_smote.predict(X_test)\n",
    "end = time()\n",
    "time_rf_smote_pred = end - start\n",
    "print(\"Testing Time: {:.2f}\".format(time_rf_smote_pred))\n",
    "# Métriques\n",
    "acc_rf_smote = metrics.accuracy_score(y_test, y_rf_smote_pred)\n",
    "print(\"Accuracy: {:.3f}\".format(acc_rf_smote))\n",
    "fbeta_rf_smote = metrics.fbeta_score(y_test, y_rf_smote_pred, beta=3.16)\n",
    "print(\"Fbeta: {:.3f}\".format(fbeta_rf_smote))\n",
    "confusion_rf_smote = metrics.confusion_matrix(y_test, y_rf_smote_pred)\n",
    "print(confusion_rf_smote)\n",
    "print(metrics.classification_report(y_test, y_rf_smote_pred))\n",
    "# Courbe ROC\n",
    "fpr, tpr, _ = metrics.roc_curve(y_test,  rf_smote.predict_proba(X_test)[:,1])\n",
    "plt.plot(fpr,tpr)\n",
    "plt.ylabel('Ratio de Vrai Positif')\n",
    "plt.xlabel('Ratio de FAux Positif')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sur le modèle de forêt aléatoire avec boost par gradient, on obtient toujours un meilleur résultat avec la méthode de sous-échantillonnage, malgré la perte d'information que cette dernière induit. Avec un score de 0.534, il est même légèrement supérieur à la méthode de regression logistique, malgré la perte d'information que cette dernière induit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balanced Random Forest avec GridSearch CV\n",
    "# Dans ce cas, c'est le jeu de données déséquilibré qui est utilisé \n",
    "# puisque ce modèle est conçu pour ce cas\n",
    "\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "# Grilles dans l'ordre de recherche\n",
    "paramGrid1 = {'max_depth': [3,4,5], 'n_jobs':[-1] , 'n_estimators':[50,100,200], \n",
    "             'random_state':[0]}\n",
    "paramGrid2 = {'n_jobs':[-1], 'max_depth': [4,5,6],\n",
    "              'n_estimators':[175,200,225], 'random_state':[0]}\n",
    "paramGrid3 = {'n_jobs':[-1] , 'max_depth': [6,7,8],\n",
    "              'n_estimators':[200], 'random_state':[0]}\n",
    "\n",
    "# Entrainement sur le jeu de données sous echantillonné\n",
    "start = time()\n",
    "rfb_imb = model_selection.GridSearchCV(BalancedRandomForestClassifier(), paramGrid1, cv=5,\n",
    "                                      scoring=target)\n",
    "rfb_imb.fit(X_train, y_train)\n",
    "end = time()\n",
    "time_rfb_grid = end - start\n",
    "print(\"Testing Time: {:.2f}\".format(time_rfb_grid))\n",
    "\n",
    "print(rfb_imb.best_params_)\n",
    "# Afficher les performances correspondantes\n",
    "print(\"Résultats de la validation croisée :\")\n",
    "for mean, std, params in zip(rfb_imb.cv_results_['mean_test_score'], \n",
    "                             rfb_imb.cv_results_['std_test_score'], rfb_imb.cv_results_['params']):\n",
    "    print(\"{} = {:.3f} (+/-{:.03f}) for {}\".format(target,mean,std,params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En première approche, on obtient les meilleurs paramètres suivants : {'max_depth': 5, 'n_estimators': 200, 'n_jobs': -1, 'random_state': 0}\n",
    "\n",
    "En deuxième approche, {'max_depth': 6, 'n_estimators': 200, 'n_jobs': -1, 'random_state': 0}\n",
    "\n",
    "En troisième approche, {'max_depth': 8, 'n_estimators': 200, 'n_jobs': -1, 'random_state': 0}\n",
    "\n",
    "Nous allons maintenant tester ces paramètres avec les 3 jeux de données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balanced Random Forest avec Gradient Boosting\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "# Entrainement sur le jeu de données déséquilibré\n",
    "rfb_imb = BalancedRandomForestClassifier(max_depth = 8, n_estimators = 200, n_jobs = -1,\n",
    "                                         random_state = 0)\n",
    "start = time()\n",
    "rfb_imb.fit(X_train, y_train)\n",
    "end = time()\n",
    "time_rfb_imb_train = end - start\n",
    "print(\"Training Time: {:.2f}\".format(time_rfb_imb_train))\n",
    "# Prédiction\n",
    "start = time()\n",
    "y_rfb_imb_pred = rfb_imb.predict(X_test)\n",
    "end = time()\n",
    "time_rfb_imb_pred = end - start\n",
    "print(\"Testing Time: {:.2f}\".format(time_rfb_imb_pred))\n",
    "# Métriques\n",
    "acc_rfb_imb = metrics.accuracy_score(y_test, y_rfb_imb_pred)\n",
    "print(\"Accuracy: {:.3f}\".format(acc_rfb_imb))\n",
    "fbeta_rfb_imb = metrics.fbeta_score(y_test, y_rfb_imb_pred, beta=3.16)\n",
    "print(\"Fbeta: {:.3f}\".format(fbeta_rfb_imb))\n",
    "confusion_rfb_imb = metrics.confusion_matrix(y_test, y_rfb_imb_pred)\n",
    "print(confusion_rfb_imb)\n",
    "print(metrics.classification_report(y_test, y_rfb_imb_pred))\n",
    "# Courbe ROC\n",
    "fpr, tpr, _ = metrics.roc_curve(y_test,  rfb_imb.predict_proba(X_test)[:,1])\n",
    "plt.plot(fpr,tpr)\n",
    "plt.ylabel('Ratio de Vrai Positif')\n",
    "plt.xlabel('Ratio de Faux Positif')\n",
    "plt.show()\n",
    "\n",
    "# Entrainement sur le jeu de données sous échantillonné\n",
    "rfb_rus = BalancedRandomForestClassifier(max_depth = 8, n_estimators = 200, n_jobs = -1,\n",
    "                                         random_state = 0)\n",
    "start = time()\n",
    "rfb_rus.fit(X_train_rus, y_train_rus)\n",
    "end = time()\n",
    "time_rfb_rus_train = end - start\n",
    "print(\"Training Time: {:.2f}\".format(time_rfb_rus_train))\n",
    "# Prédiction\n",
    "start = time()\n",
    "y_rfb_rus_pred = rfb_rus.predict(X_test)\n",
    "end = time()\n",
    "time_rfb_rus_pred = end - start\n",
    "print(\"Testing Time: {:.2f}\".format(time_rfb_rus_pred))\n",
    "# Métriques\n",
    "acc_rfb_rus = metrics.accuracy_score(y_test, y_rfb_rus_pred)\n",
    "print(\"Accuracy: {:.3f}\".format(acc_rfb_rus))\n",
    "fbeta_rfb_rus = metrics.fbeta_score(y_test, y_rfb_rus_pred, beta=3.16)\n",
    "print(\"Fbeta: {:.3f}\".format(fbeta_rfb_rus))\n",
    "confusion_rfb_rus = metrics.confusion_matrix(y_test, y_rfb_rus_pred)\n",
    "print(confusion_rfb_rus)\n",
    "print(metrics.classification_report(y_test, y_rfb_rus_pred))\n",
    "# Courbe ROC\n",
    "fpr, tpr, _ = metrics.roc_curve(y_test,  rfb_rus.predict_proba(X_test)[:,1])\n",
    "plt.plot(fpr,tpr)\n",
    "plt.ylabel('Ratio de Vrai Positif')\n",
    "plt.xlabel('Ratio de FAux Positif')\n",
    "plt.show()\n",
    "\n",
    "# Entrainement sur le jeu de données sur-échantillonné\n",
    "rfb_smote = BalancedRandomForestClassifier(max_depth = 8, n_estimators = 200, n_jobs = -1,\n",
    "                                         random_state = 0)\n",
    "start = time()\n",
    "rfb_smote.fit(X_train_smote, y_train_smote)\n",
    "end = time()\n",
    "time_rfb_smote_train = end - start\n",
    "print(\"Training Time: {:.2f}\".format(time_rfb_smote_train))\n",
    "# Prédiction\n",
    "start = time()\n",
    "y_rfb_smote_pred = rfb_smote.predict(X_test)\n",
    "end = time()\n",
    "time_rfb_smote_pred = end - start\n",
    "print(\"Testing Time: {:.2f}\".format(time_rfb_smote_pred))\n",
    "# Métriques\n",
    "acc_rfb_smote = metrics.accuracy_score(y_test, y_rfb_smote_pred)\n",
    "print(\"Accuracy: {:.3f}\".format(acc_rfb_smote))\n",
    "fbeta_rfb_smote = metrics.fbeta_score(y_test, y_rfb_smote_pred, beta=3.16)\n",
    "print(\"Fbeta: {:.3f}\".format(fbeta_rfb_smote))\n",
    "confusion_rfb_smote = metrics.confusion_matrix(y_test, y_rfb_smote_pred)\n",
    "print(confusion_rfb_smote)\n",
    "print(metrics.classification_report(y_test, y_rfb_smote_pred))\n",
    "# Courbe ROC\n",
    "fpr, tpr, _ = metrics.roc_curve(y_test,  rfb_smote.predict_proba(X_test)[:,1])\n",
    "plt.plot(fpr,tpr)\n",
    "plt.ylabel('Ratio de Vrai Positif')\n",
    "plt.xlabel('Ratio de FAux Positif')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthèse\n",
    "Aucune courbe ROC ne montre un coude marqué suggerant une possible optimisation du seuil de détection. De plus, ces courbes étant calculées sur le jeu de données de test, il ne faut pas oublier que toute optimisation se fera donc à posteriori du test, ce qui crée un biais et risque de provoquer de l'overfitting.\n",
    "\n",
    "En se basant sur le Fbeta score et sur les détails de la classification, le modèle optimal semble être la forêt aléatoire avec boosting par gradient sur le jeu de données sous-échantillonné. Si l'on se réfère à la matrice de confusion, on a 27% de FP ce qui est un point négatif non négligeable mais on réussi à repérer 69% de TP ce qui est plutôt élevé considérant que ce n'est pas un problème de classification aisé (cf Visualisation tsne)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test de la fonction class_weight, cette fonction n'existe pas pour GradientBoostingClassifier,\n",
    "# notre modèle optimal, mais on peut la tester sur LogisticRegression\n",
    "\n",
    "# Regression Logistique\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# Entrainement sur le jeu de données déséquilibré\n",
    "regCW_imb = LogisticRegression(penalty = 'l2', solver = 'liblinear', random_state = 0,\n",
    "                             max_iter = 1000, class_weight='balanced', n_jobs = -1)\n",
    "start = time()\n",
    "regCW_imb.fit(X_train, y_train)\n",
    "end = time()\n",
    "time_regCW_imb_train = end - start\n",
    "print(\"Training Time: {:.2f}\".format(time_regCW_imb_train))\n",
    "# Prédiction\n",
    "start = time()\n",
    "y_regCW_imb_pred = regCW_imb.predict(X_test)\n",
    "end = time()\n",
    "time_regCW_imb_pred = end - start\n",
    "print(\"Testing Time: {:.2f}\".format(time_regCW_imb_pred))\n",
    "# Métriques\n",
    "acc_regCW_imb = metrics.accuracy_score(y_test, y_regCW_imb_pred)\n",
    "print(\"Accuracy: {:.3f}\".format(acc_regCW_imb))\n",
    "fbeta_regCW_imb = metrics.fbeta_score(y_test, y_regCW_imb_pred, beta=3.16)\n",
    "print(\"Fbeta: {:.3f}\".format(fbeta_regCW_imb))\n",
    "confusion_regCW_imb = metrics.confusion_matrix(y_test, y_regCW_imb_pred)\n",
    "print(confusion_regCW_imb)\n",
    "print(metrics.classification_report(y_test, y_regCW_imb_pred))\n",
    "# Courbe ROC\n",
    "fpr, tpr, _ = metrics.roc_curve(y_test,  regCW_imb.predict_proba(X_test)[:,1])\n",
    "plt.plot(fpr,tpr)\n",
    "plt.ylabel('Ratio de Vrai Positif')\n",
    "plt.xlabel('Ratio de Faux Positif')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En utilisant class_weight='balanced' sur le jeu de données déséquilibrées, on obtient un résultat de fbeta_score bien plus élevé, il s'avère donc que ce ne sont pas deux méthodes redondantes. Ici on obtient des résultats très très proches du même modèle sans class_weight sur les données sous-échantillonnées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tableau des résultats des modèles optimisés Knn\n",
    "from pandas.plotting import table\n",
    "synthese_knn = pd.DataFrame()\n",
    "synthese_knn = synthese_knn.assign(Dataset = ['Imabalanced', 'Undersampled','Smote'])\n",
    "synthese_knn = synthese_knn.assign(Accuracy = [acc_knn_imb,acc_knn_rus,acc_knn_smote])\n",
    "synthese_knn = synthese_knn.assign(Fbeta = [fbeta_knn_imb,fbeta_knn_rus,fbeta_knn_smote])\n",
    "synthese_knn = synthese_knn.assign(TimeTrain = [time_knn_imb_train,time_knn_rus_train,\n",
    "                                                time_knn_smote_train])\n",
    "synthese_knn = synthese_knn.assign(TimeTest = [time_knn_imb_pred,time_knn_rus_pred,time_knn_smote_pred])\n",
    "print(synthese_knn)\n",
    "\n",
    "# Affichage sous forme de tableau à télécharger\n",
    "plt.figure(figsize=(8,6))\n",
    "# Pas d'axes\n",
    "ax = plt.subplot(frame_on=False)\n",
    "ax.xaxis.set_visible(False)\n",
    "ax.yaxis.set_visible(False)\n",
    "#Création du tableau\n",
    "table(ax, synthese_knn.round(decimals=3))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tableau des résultats des modèles optimisés Regression Logistique\n",
    "synthese_reg = pd.DataFrame()\n",
    "synthese_reg = synthese_reg.assign(Dataset = ['Imabalanced', 'Undersampled','Smote','Imbalanced CW'])\n",
    "synthese_reg = synthese_reg.assign(Accuracy = [acc_reg_imb,acc_reg_rus,acc_reg_smote, acc_regCW_imb])\n",
    "synthese_reg = synthese_reg.assign(Fbeta = [fbeta_reg_imb,fbeta_reg_rus,fbeta_reg_smote,\n",
    "                                            fbeta_regCW_imb])\n",
    "synthese_reg = synthese_reg.assign(TimeTrain = [time_reg_imb_train,time_reg_rus_train, \n",
    "                                                time_reg_smote_train, time_regCW_imb_train])\n",
    "synthese_reg = synthese_reg.assign(TimeTest = [time_reg_imb_pred,time_reg_rus_pred,time_reg_smote_pred,\n",
    "                                               time_regCW_imb_pred])\n",
    "print(synthese_reg)\n",
    "\n",
    "# Affichage sous forme de tableau à télécharger\n",
    "plt.figure(figsize=(8,6))\n",
    "# Pas d'axes\n",
    "ax = plt.subplot(frame_on=False)\n",
    "ax.xaxis.set_visible(False)\n",
    "ax.yaxis.set_visible(False)\n",
    "#Création du tableau\n",
    "table(ax, synthese_knn.round(decimals=3))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tableau des résultats des modèles optimisés Random Forest\n",
    "synthese_rf = pd.DataFrame()\n",
    "synthese_rf = synthese_rf.assign(Dataset = ['Imabalanced', 'Undersampled','Smote'])\n",
    "synthese_rf = synthese_rf.assign(Accuracy = [acc_rf_imb,acc_rf_rus,acc_rf_smote])\n",
    "synthese_rf = synthese_rf.assign(Fbeta = [fbeta_rf_imb,fbeta_rf_rus,fbeta_rf_smote])\n",
    "synthese_rf = synthese_rf.assign(TimeTrain = [time_rf_imb_train,time_rf_rus_train,time_rf_smote_train])\n",
    "synthese_rf = synthese_rf.assign(TimeTest = [time_rf_imb_pred,time_rf_rus_pred,time_rf_smote_pred])\n",
    "print(synthese_rf)\n",
    "\n",
    "# Affichage sous forme de tableau à télécharger\n",
    "plt.figure(figsize=(8,6))\n",
    "# Pas d'axes\n",
    "ax = plt.subplot(frame_on=False)\n",
    "ax.xaxis.set_visible(False)\n",
    "ax.yaxis.set_visible(False)\n",
    "#Création du tableau\n",
    "table(ax, synthese_knn.round(decimals=3))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tableau des résultats des modèles optimisés Balanced Random Forest\n",
    "synthese_rfb = pd.DataFrame()\n",
    "synthese_rfb = synthese_rfb.assign(Dataset = ['Imabalanced', 'Undersampled','Smote'])\n",
    "synthese_rfb = synthese_rfb.assign(Accuracy = [acc_rfb_imb,acc_rfb_rus,acc_rfb_smote])\n",
    "synthese_rfb = synthese_rfb.assign(Fbeta = [fbeta_rfb_imb,fbeta_rfb_rus,fbeta_rfb_smote])\n",
    "synthese_rfb = synthese_rfb.assign(TimeTrain = [time_rfb_imb_train,time_rfb_rus_train,time_rfb_smote_train])\n",
    "synthese_rfb = synthese_rfb.assign(TimeTest = [time_rfb_imb_pred,time_rfb_rus_pred,time_rfb_smote_pred])\n",
    "print(synthese_rfb)\n",
    "\n",
    "# Affichage sous forme de tableau à télécharger\n",
    "plt.figure(figsize=(8,6))\n",
    "# Pas d'axes\n",
    "ax = plt.subplot(frame_on=False)\n",
    "ax.xaxis.set_visible(False)\n",
    "ax.yaxis.set_visible(False)\n",
    "#Création du tableau\n",
    "table(ax, synthese_knn.round(decimals=3))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Time: 146.70\n",
      "Testing Time: 0.83\n",
      "Accuracy: 0.694\n",
      "Fbeta: 0.534\n",
      "[[58912 26002]\n",
      " [ 2271  5069]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.69      0.81     84914\n",
      "         1.0       0.16      0.69      0.26      7340\n",
      "\n",
      "    accuracy                           0.69     92254\n",
      "   macro avg       0.56      0.69      0.54     92254\n",
      "weighted avg       0.90      0.69      0.76     92254\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "rf_rus = GradientBoostingClassifier(learning_rate = 0.12, loss = 'exponential', max_depth = 4, \n",
    "                                    n_estimators = 250, random_state = 0)\n",
    "start = time()\n",
    "rf_rus.fit(X_train_rus, y_train_rus)\n",
    "end = time()\n",
    "time_rf_rus_train = end - start\n",
    "print(\"Training Time: {:.2f}\".format(time_rf_rus_train))\n",
    "# Prédiction\n",
    "start = time()\n",
    "y_rf_rus_pred = rf_rus.predict(X_test)\n",
    "end = time()\n",
    "time_rf_rus_pred = end - start\n",
    "print(\"Testing Time: {:.2f}\".format(time_rf_rus_pred))\n",
    "# Métriques\n",
    "acc_rf_rus = metrics.accuracy_score(y_test, y_rf_rus_pred)\n",
    "print(\"Accuracy: {:.3f}\".format(acc_rf_rus))\n",
    "fbeta_rf_rus = metrics.fbeta_score(y_test, y_rf_rus_pred, beta=3.16)\n",
    "print(\"Fbeta: {:.3f}\".format(fbeta_rf_rus))\n",
    "confusion_rf_rus = metrics.confusion_matrix(y_test, y_rf_rus_pred)\n",
    "print(confusion_rf_rus)\n",
    "print(metrics.classification_report(y_test, y_rf_rus_pred))\n",
    "# Courbe ROC\n",
    "fpr, tpr, _ = metrics.roc_curve(y_test,  rf_rus.predict_proba(X_test)[:,1])\n",
    "plt.plot(fpr,tpr)\n",
    "plt.ylabel('Ratio de Vrai Positif')\n",
    "plt.xlabel('Ratio de FAux Positif')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model_rf.pkl']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Persistance du modèle\n",
    "import joblib\n",
    "joblib.dump(rf_rus, 'model_rf.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création d'un échantillon de données réduit pour le cloud\n",
    "# Pour la prédiction (normalisé)\n",
    "# On réintègre le SK_ID_CURR\n",
    "data = data.join(SK_ID_CURR)\n",
    "data_sampled = data.sample(frac=0.09, random_state=0)\n",
    "data_sampled.to_csv(os.path.join(output_dir,'data_sampled.csv'), index=False)\n",
    "# Pour l'affichage de graphiques\n",
    "data_chart = pd.read_csv(os.path.join(default_dir,'data_train.csv'))\n",
    "data_chart_sampled = data_chart.sample(frac=0.09, random_state=0)\n",
    "data_chart_sampled.to_csv(os.path.join(output_dir,'data_chart_sampled.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.join(SK_ID_CURR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data['SK_ID_CURR']==100003]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Format de la Cellule Texte Brut",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
